{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4adc7403",
   "metadata": {},
   "source": [
    "# Exploring parameters\n",
    "\n",
    "#### density plots, post predictive checks etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e788d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os.path as op\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import hnn_core\n",
    "from hnn_core import simulate_dipole, jones_2009_model\n",
    "from hnn_core.viz import plot_dipole\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../code/')\n",
    "sys.path.append('../code/utils/')\n",
    "#sys.path.append('../code/sbi/')\n",
    "#sys.path.append('../../results_cluster/')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "work_dir = '/home/ubuntu/sbi_for_eeg_data/code/'\n",
    "\n",
    "os.chdir(work_dir)\n",
    "\n",
    "#from utils.plot import cov, compare_vars, plot_varchanges\n",
    "#from utils.plot import compare_KLs, plot_KLs\n",
    "#from sbi.inference import potentials\n",
    "import utils.sbi_modulated_functions\n",
    "\n",
    "# sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "\n",
    "from sbi.analysis import conditional_pairplot, conditional_corrcoeff\n",
    "\n",
    "\n",
    "\n",
    "# import the summary statistics that you want to investigate\n",
    "from summary_features.calculate_summary_features import calculate_summary_statistics_alternative as alternative_sumstats\n",
    "from summary_features.calculate_summary_features import calculate_summary_stats_temporal as temporal_sumstats\n",
    "from summary_features.calculate_summary_features import calculate_summary_stats_number as number_sumstats\n",
    "from summary_features.calculate_summary_features import calculate_summary_stats_temporal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab993458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c212ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## defining neuronal network model\n",
    "\n",
    "num_params = 17\n",
    "\n",
    "from utils.simulation_wrapper import set_network_default, SimulationWrapper\n",
    "sim_wrapper = SimulationWrapper(num_params, noise=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b287160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_len = 30\n",
    "\n",
    "if num_params == 6:\n",
    "    \n",
    "    prior_min_fix = [0.0, 11.3, 0.0, 43.8, 0.0, 89.491]\n",
    "    prior_max_fix = [0.160, 35.9, 0.821, 79.0, 8.104, 162.110]\n",
    "\n",
    "    prior_min = [0.0, 11.3, 0.0, 43.8, 0.0, 89.491]\n",
    "    prior_max = [0.160, 35.9, 0.821, 79.0, 8.104, 162.110]\n",
    "    #true_params = torch.tensor([[26.61, 63.53,  137.12]])\n",
    "    true_params = torch.tensor([[0.0274, 19.01, 0.1369, 61.89, 0.1435, 120.86]])\n",
    "\n",
    "    \n",
    "\n",
    "if num_params == 25:\n",
    "    prior_min = [0, 0, 0, 0, 0, 0, 0, 0, 13.3,    # prox1 weights\n",
    "                 0, 0, 0, 0, 0, 0, 51.980,            # distal weights\n",
    "                 0, 0, 0, 0, 0, 0, 0, 0, 112.13]       # prox2 weights\n",
    "\n",
    "\n",
    "    # ampa, nmda [0.927, 0.160, 2.093, 0.0519,        1.0, 1.0, 1.0, 1.0, 35.9,\n",
    "    #           0.0394, 0.000042, 0.039372,           0.854, 0.117,  0.480, 75.08,\n",
    "    #            0.000018, 8.633, 0.05375, 4.104,     1.0, 1.0, 1.0, 1.0, 162.110]\n",
    "\n",
    "    prior_max = [0.927, 1.0, 0.160, 1.0,  2.093, 1.0, 0.0519, 1.0, 35.9,\n",
    "                 0.0394, 0.117, 0.000042, 0.025902, 0.854, 0.480, 75.08,\n",
    "                 0.000018, 1.0, 8.633, 1.0, 0.05375, 1.0, 4.104,  1.0, 162.110]\n",
    "\n",
    "    true_params = torch.tensor([[0.277, 0.3739, 0.0399, 0.0, 0.6244, 0.3739, 0.034, 0.0, 18.977,\n",
    "                                 0.011467, 0.06337, 0.000012, 0.013407, 0.466095, 0.0767, 63.08,\n",
    "                                 0.000005, 0.116706, 4.6729, 0.016733, 0.011468, 0.061556, 2.33, 0.0679, 120.86]])\n",
    "    \n",
    "if num_params == 17:\n",
    "    \n",
    "    prior_min = [0, 0, 0, 0, 0, 13.3,  0, 0, 0, 0, 0, 51.980, 0, 0, 0, 0, 112.13]\n",
    "    prior_max = [0.927, 0.160, 2.093, 1.0, 1.0, 35.9, 0.000042, 0.039372, 0.025902,  0.480, 0.117, 75.08, 8.633, 4.104, 1.0, 1.0, 162.110]\n",
    "\n",
    "    true_params = torch.tensor([[0.277, 0.0399, 0.6244, 0.3739, 0.0, 18.977, 0.000012, 0.0115, 0.0134,  0.0767, 0.06337, 63.08, 4.6729, 2.33, 0.016733, 0.0679, 120.86]])\n",
    "\n",
    "\n",
    "prior = utils.torchutils.BoxUniform(low=prior_min, high=prior_max)\n",
    "\n",
    "#number_simulations = 10\n",
    "density_estimator = 'nsf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5528677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#assert (prior.event_shape==torch.Size([25]))\n",
    "from utils import inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "230edc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\n",
      "/home/ubuntu/results\n"
     ]
    }
   ],
   "source": [
    "from utils import inference\n",
    "\n",
    "import pickle\n",
    "from data_load_writer import *\n",
    "from data_load_writer import load_from_file as lf\n",
    "\n",
    "import os\n",
    "\n",
    "work_dir = '/home/ubuntu/'\n",
    "\n",
    "os.chdir(work_dir)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "file = 'multi_round'\n",
    "\n",
    "#file = 'multi_round_10000_17params_nsf_fake_2104_001_std'\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "os.chdir('results')\n",
    "\n",
    "\n",
    "print(os.getcwd())\n",
    "  \n",
    "\n",
    "#thetas = torch.load('{}/thetas.pt'.format(file))\n",
    "\n",
    "posterior = torch.load('{}/posterior.pt'.format(file))\n",
    "\n",
    "#neural_dens = torch.load('{}/neural_dens.pt'.format(file))\n",
    "#x_without = torch.load('{}/obs_without.pt'.format(file))\n",
    "\n",
    "#x = calculate_summary_stats_temporal(x_without)\n",
    "\n",
    "\n",
    "#true_params = torch.tensor([[0.0274, 19.01, 0.1369, 61.89, 0.1435, 120.86]])\n",
    "#true_params = torch.tensor([[  18.9700, 63.5300, 137.1200]])\n",
    "#true_params = torch.load('results/{}/true_params.pt'.format(file))\n",
    "#true_params = torch.tensor([[0.277, 0.0399, 0.3739, 0.034, 18.977, 0.0115, 0.000012, 0.466, 0.06337, 0.0134, 0.0766, 63.08, 0.000005, 4.6729, 0.0115, 0.3308, 120.86]])\n",
    "\n",
    "#posteriors_round =  torch.load('{}/posteriors_each_round.pt'.format(file))\n",
    "#obs_real = torch.load('{}/obs_real.pt'.format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6f7411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#posterior = posteriors_round[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20447a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81dd379447c44c948df79e32f18808fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_833853/3182104500.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#obs_real_stat2 = calculate_summary_stats_temporal(obs_real[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposterior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_within_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/sbi/inference/posteriors/direct_posterior.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_shape, x, show_progress_bars, sample_with, mcmc_method, mcmc_parameters, rejection_sampling_parameters, sample_with_mcmc, not_within_prior)\u001b[0m\n\u001b[1;32m    394\u001b[0m                     \u001b[0msample_for_correction_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                     \u001b[0mnot_within_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnot_within_prior\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0mrejection_sampling_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m                 )[0]\n\u001b[1;32m    398\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/sbi/utils/sbiutils.py\u001b[0m in \u001b[0;36mrejection_sample_posterior_within_prior\u001b[0;34m(posterior_nn, prior, x, num_samples, show_progress_bars, warn_acceptance, sample_for_correction_factor, max_sampling_batch_size, not_within_prior, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Sample and reject.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         candidates = posterior_nn.sample(sampling_batch_size, context=x).reshape(\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0msampling_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/distributions/base.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, context, batch_size)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/flows/base.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, num_samples, context)\u001b[0m\n\u001b[1;32m     52\u001b[0m             )\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedded_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedded_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_logabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_logabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mfuncs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/base.py\u001b[0m in \u001b[0;36m_cascade\u001b[0;34m(inputs, funcs, context)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfuncs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogabsdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mtotal_logabsdet\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlogabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_logabsdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/transforms/coupling.py\u001b[0m in \u001b[0;36minverse\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m    116\u001b[0m             )\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtransform_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentity_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         transform_split, logabsdet_split = self._coupling_transform_inverse(\n\u001b[1;32m    120\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/nn/nets/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/nflows/nn/nets/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, context)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mtemps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sbi_env/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#obs_real_stat2 = calculate_summary_stats_temporal(obs_real[0])\n",
    "\n",
    "samples = posterior.sample((1000,), not_within_prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = [\"prox1_ampa_l2_bas\",\"prox1_ampa_l2_pyr\",\"prox1_ampa_l5_bas\",\"prox1_nmda_l5_bas\", \"prox1_nmda_l5_pyr\",\n",
    " \"t_prox1\",\n",
    " \"dist_ampa_l2_pyr\",\"dist_ampa_l2_bas\",\"dist_nmda_l2_pyr\",\n",
    " \"dist_nmda_l5_pyr\",\"dist_nmda_l2_bas\",\n",
    " \"t_dist\", \n",
    " \"prox2_ampa_l2_pyr\",\"prox2_ampa_l5_pyr\",\"prox2_nmda_l2_pyr\",\"prox2_nmda_l5_pyr\",\n",
    " \"t_prox2\"]\n",
    "\n",
    "if num_params ==6:\n",
    "    parameter_names = [\"prox_1_ampa_l2_pyr\",\n",
    "     \"t_evprox_1\",\n",
    "     \"dist_nmda_l2_pyr\",\n",
    "     \"t_evdist_1\", \n",
    "     \"prox_2_ampa_l5_pyr\",\n",
    "     \"t_evprox_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a180df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##better limits:\n",
    "\n",
    "list_min = torch.min(samples, 0)[0]\n",
    "list_max = torch.max(samples, 0)[0]\n",
    "\n",
    "print(list_min)\n",
    "\n",
    "print(list_max)\n",
    "\n",
    "diff = torch.abs(list_max - list_min) * 0.1\n",
    "\n",
    "print(diff)\n",
    "\n",
    "list_min = list(torch.nn.functional.relu(list_min) - diff)\n",
    "#list_min = list(list_min - diff)\n",
    "list_max = list(list_max - 2* diff)\n",
    "\n",
    "limits = [list(tup) for tup in zip(list_min, list_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b787dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "limits = [list(tup) for tup in zip(prior_min, prior_max)]\n",
    "\n",
    "\n",
    "plt.set_cmap('viridis')\n",
    "\n",
    "fig, axes = analysis.pairplot(\n",
    "    samples,\n",
    "    limits=limits,\n",
    "    upper = 'kde',\n",
    "    lower='kde',\n",
    "    #subset=[5, 11, 16],\n",
    "    ticks=np.round(limits,2),\n",
    "    figsize=(20, 20),\n",
    "    points=true_params,\n",
    "    points_offdiag={\"markersize\": 5},\n",
    "    points_colors=\"r\",\n",
    "    labels=parameter_names,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#for i in range(5):\n",
    "#    axes[i][i].xaxis.label.set_color('magenta')\n",
    "#for i in range(5, 12):\n",
    "#    axes[i][i].xaxis.label.set_color('navy')\n",
    "#for i in range(12, 17):\n",
    "#    axes[i][i].xaxis.label.set_color('deeppink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fe1f12f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix_marginal = np.corrcoef(samples.T)\n",
    "fig, ax = plt.subplots(1,1, figsize=(4, 4))\n",
    "im = plt.imshow(corr_matrix_marginal, clim=[-1, 1], cmap='PiYG')\n",
    "_ = fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3bf4664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8432da46924756aba29faf97d1c7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  2.499937501562461e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5212060f26ba44819975d2c5a2904534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  3.3332962967078146e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38178b005ac645798d01ce4770d2bc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  1.999960000799984e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85f6972638342bfb1f4fe2e57161266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  1.1110987655692714e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0dff5e813d4f9cbe5755d807eb9ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  3.333222225925803e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c63b182de084764a6f9644937631076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  1.999960000799984e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9eebd4f500426a9b9e07f5928367e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 0 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  9.999000099990002e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c027445ad0d472fa80aace6fc52ebba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 0 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  9.999000099990002e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dffa881e20941f6a24fea01e86eb935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 1 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  0.0\n",
      "acceptance rate:  3.333222225925803e-05\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c23c3d18a14b67a5caa97d2aa98a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1 posterior samples:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Drawing samples from posterior to estimate the normalizing\n",
      "                        constant for `log_prob()`. However, only\n",
      "                        0% posterior samples are within the\n",
      "                        prior support. It may take a long time to collect the\n",
      "                        remaining 0 samples.\n",
      "                        Consider interrupting (Ctrl-C) and either basing the\n",
      "                        estimate of the normalizing constant on fewer samples (by\n",
      "                        calling `posterior.leakage_correction(x_o,\n",
      "                        num_rejection_samples=N)`, where `N` is the number of\n",
      "                        samples you want to base the\n",
      "                        estimate on (default N=10000), or not estimating the\n",
      "                        normalizing constant at all\n",
      "                        (`log_prob(..., norm_posterior=False)`. The latter will\n",
      "                        result in an unnormalized `log_prob()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acceptance rate:  0.0\n",
      "acceptance rate:  9.999000099990002e-05\n"
     ]
    }
   ],
   "source": [
    "cond_coeff_mat_list = []\n",
    "\n",
    "\n",
    "## for the conditional correlation matrix, we had to take the posterior from the second round because leakage was too huge \n",
    "## for the third round\n",
    "\n",
    "posteriors_round =  torch.load('{}/posteriors_each_round.pt'.format(file))\n",
    "\n",
    "posterior = posteriors_round[1]\n",
    "\n",
    "for i in range(10):\n",
    "    condition = posterior.sample((1,))\n",
    "    cond_coeff_mat = conditional_corrcoeff(\n",
    "        density=posterior,\n",
    "        condition=condition,\n",
    "        limits=torch.tensor([[-2., 2.]]*17),\n",
    "    )\n",
    "    cond_coeff_mat_list.append(cond_coeff_mat)\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfbdb3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cond_coeff_mat_tensor = torch.stack(cond_coeff_mat_list, dim=0)\n",
    "\n",
    "#take average:\n",
    "cond_coeff_mat_av = torch.nanmean(cond_coeff_mat_tensor, dim=0)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(4,4))\n",
    "im = plt.imshow(cond_coeff_mat_av, clim=[-1, 1], cmap='PiYG')\n",
    "_ = fig.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e830bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
