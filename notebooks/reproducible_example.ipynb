{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ecef64",
   "metadata": {},
   "source": [
    "## Toy example: Inferring the mean of Gaussians\n",
    "\n",
    "#### comparing the multi-round SNPE approach against our new incremental approach.\n",
    "\n",
    "Goal of this little toy example is to show that provided our parameters are independent of each other, we need less simulations to derive a good approximation of our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd32f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code/')\n",
    "\n",
    "import utils\n",
    "from utils.helpers import get_time\n",
    "from utils import inference\n",
    "\n",
    "from utils.sbi_modulated_functions import Combined\n",
    "\n",
    "\n",
    "from utils.helpers import get_time\n",
    "\n",
    "\n",
    "\n",
    "# sbi\n",
    "from sbi import utils as utils\n",
    "from sbi import analysis as analysis\n",
    "from sbi.inference.base import infer\n",
    "from sbi.inference import SNPE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi.inference import SNPE_C\n",
    "\n",
    "import sbi\n",
    "\n",
    "import pyknos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecce96e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb05c0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17.2\n"
     ]
    }
   ],
   "source": [
    "print(sbi.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53c8ed3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "def Gaussian(thetas, normal_noise=0.1):\n",
    "    \n",
    "    gauss_list = []\n",
    "    \n",
    "    for theta in thetas:\n",
    "    \n",
    "        mu, sigma = theta, normal_noise # mean and standard deviation\n",
    "\n",
    "        s = np.random.normal(mu, sigma, 1)\n",
    "        \n",
    "        gauss_list.append(s[0])\n",
    "        \n",
    "    gauss_obs = torch.tensor(gauss_list)\n",
    "    \n",
    "    return gauss_obs\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8d549",
   "metadata": {},
   "source": [
    "### Calculate posterior for different number of simulations: 1k,  3k, 5k, 10k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06b59c",
   "metadata": {},
   "source": [
    "### starting with multi-round snpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04a908c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n"
     ]
    }
   ],
   "source": [
    "true_thetas = torch.tensor([[3.0, 6.0, 20.0, 10.0, 90.0, 55.0, 27.0, 29.0, 4.0, 70.0, 5.0, 66.0, 99.0, 40.0, 45.0]], dtype=torch.float32)\n",
    "parameter_names = ['t1', 't2', 't3', 't4', 't5', 't6', 't7', 't8', 't9', 't10', 't11', 't12', 't13', 't14', 't15']\n",
    "\n",
    "prior_max = [100.0] * 15\n",
    "prior_min = [0.1] * 15\n",
    "\n",
    "print(prior_max)\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fa2eaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3.,  6., 20., 10., 90., 55., 27., 29.,  4., 70.,  5., 66., 99., 40.,\n",
      "         45.]])\n"
     ]
    }
   ],
   "source": [
    "print(true_thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f2af52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 3.0500,  5.9777, 19.9744,  9.9116, 90.0515, 55.3153, 27.0714, 29.0864,\n",
      "         3.8423, 70.0605,  4.9874, 65.8916, 99.1237, 40.0247, 44.9965],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea44b02193724d8a987faf37f0cf7da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 500 simulations in 500 batches.:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      " Neural network successfully converged after 56 epochs.for round  0  time is:  0:00:17.843562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7b8a249e94314a08647139141f024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 500 posterior samples:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57b371b1f6b4ccbb8033016e9922121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 500 simulations in 500 batches.:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SNPE-C with non-atomic loss\n",
      " Neural network successfully converged after 25 epochs.for round  1  time is:  0:00:27.179489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a4bb32ae074491a6954b00400e1e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 500 posterior samples:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e82fb3e004748ab9be0e749831e4e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 500 simulations in 500 batches.:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SNPE-C with non-atomic loss\n",
      " Neural network successfully converged after 25 epochs.for round  2  time is:  0:00:28.425134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852db2a256da4ef48c8abd4c62907a49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 1000 simulations in 1000 batches.:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      " Neural network successfully converged after 59 epochs.for round  0  time is:  0:00:36.418636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cb1d6c07f94429ac7c05138303177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Drawing 1000 posterior samples:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250b3c9b9e5b484dbb1a52b0cc183971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 1000 simulations in 1000 batches.:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SNPE-C with non-atomic loss\n",
      " Training neural network. Epochs trained: 21"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "NaN/Inf present in proposal posterior eval.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3802509/818299026.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0minf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend_simulations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproposal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mdensity_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mposterior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_posterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdensity_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sbi/sbi/inference/snpe/snpe_c.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_atoms, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, exclude_invalid_x, resume_training, discard_prior_samples, use_combined_loss, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_state_for_mog_proposal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_state_for_mog_proposal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sbi/sbi/inference/snpe/snpe_base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_batch_size, learning_rate, validation_fraction, stop_after_epochs, max_num_epochs, clip_max_norm, calibration_kernel, exclude_invalid_x, resume_training, discard_prior_samples, retrain_from_scratch, show_train_summary, dataloader_kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 train_losses = self._loss(\n\u001b[0;32m--> 286\u001b[0;31m                     \u001b[0mtheta_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibration_kernel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m                 )\n\u001b[1;32m    288\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sbi/sbi/inference/snpe/snpe_base.py\u001b[0m in \u001b[0;36m_loss\u001b[0;34m(self, theta, x, masks, proposal, calibration_kernel)\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_neural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob_proposal_posterior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibration_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sbi/sbi/inference/snpe/snpe_c.py\u001b[0m in \u001b[0;36m_log_prob_proposal_posterior\u001b[0;34m(self, theta, x, masks, proposal)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_non_atomic_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob_proposal_posterior_mog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_prob_proposal_posterior_atomic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sbi/sbi/inference/snpe/snpe_c.py\u001b[0m in \u001b[0;36m_log_prob_proposal_posterior_mog\u001b[0;34m(self, theta, x, proposal)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_pp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec_pp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         )\n\u001b[0;32m--> 413\u001b[0;31m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob_proposal_posterior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"proposal posterior eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob_proposal_posterior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sbi/sbi/utils/torchutils.py\u001b[0m in \u001b[0;36massert_all_finite\u001b[0;34m(quantity, description)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"NaN/Inf present in {description}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: NaN/Inf present in proposal posterior eval."
     ]
    }
   ],
   "source": [
    "num_simulations_list = [500, 1000]\n",
    "\n",
    "\n",
    "posterior_snpe_list = []\n",
    "\n",
    "obs_real = Gaussian(true_thetas[0])\n",
    "\n",
    "#obs_real = torch.tensor(obs_real, dtype=torch.float32)\n",
    "\n",
    "print(obs_real)\n",
    "\n",
    "for num_simulations in num_simulations_list:\n",
    "    \n",
    "    \n",
    "    prior = utils.torchutils.BoxUniform(low=prior_min, high = prior_max)\n",
    "    inf = SNPE(prior, density_estimator=\"mdn\")\n",
    "    simulator_stats, prior = prepare_for_sbi(Gaussian, prior)\n",
    "\n",
    "    proposal = prior\n",
    "    \n",
    "\n",
    "    for i in range(3):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        theta, x = simulate_for_sbi(\n",
    "            simulator_stats,\n",
    "            proposal=proposal,\n",
    "            num_simulations=num_simulations,\n",
    "            num_workers=8,\n",
    "        )\n",
    "\n",
    "        inf = inf.append_simulations(theta, x, proposal=proposal)\n",
    "        density_estimator = inf.train()\n",
    "\n",
    "        posterior = inf.build_posterior(density_estimator)\n",
    "\n",
    "\n",
    "\n",
    "        proposal = posterior.set_default_x(obs_real)\n",
    "        \n",
    "        finish_time = datetime.datetime.now()\n",
    "\n",
    "        diff_time_snpe = finish_time - start_time\n",
    "        \n",
    "        print('for round ',i, ' time is: ', diff_time_snpe)\n",
    "\n",
    "    posterior_snpe = posterior\n",
    "    \n",
    "    posterior_snpe_list.append(posterior_snpe)\n",
    "    \n",
    "finish_time = datetime.datetime.now()\n",
    "\n",
    "diff_time_snpe = finish_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc1fc433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:57:44.220780\n"
     ]
    }
   ],
   "source": [
    "print(diff_time_snpe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
